{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from cleantext import clean\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content without empty:  1430252\n",
      "Content without Nan and unknown:  1377086\n",
      "Content without duplicates:  932256\n"
     ]
    }
   ],
   "source": [
    "end_result = pd.read_csv('news_cleaned_2018_02_13.csv', encoding='utf8', nrows=1_600_000, dtype={'content':'string', 'type':'string'}, usecols=['content', 'type'], lineterminator='\\n', skip_blank_lines=True).dropna()\n",
    "print(\"Content without empty: \", len(end_result.index))\n",
    "end_result = end_result[~end_result['type'].isin(['nan', 'unknown'])]\n",
    "print(\"Content without Nan and unknown: \", len(end_result.index))\n",
    "end_result = end_result.drop_duplicates(subset=['content'])\n",
    "print(\"Content without duplicates: \", len(end_result.index))\n",
    "end_result.to_csv('content_type_data.csv', columns=['content', 'type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def clean_data(input_text):\n",
    "    cleaned_text = re.sub(r'(\\S+\\.com*\\S+)', '<url>', input_text)\n",
    "    cleaned_text = re.sub(r'(\\S+\\.net*\\S+)', '<url>', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\-', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\|', ' ', cleaned_text)\n",
    "    cleaned_text = clean(cleaned_text,  # does not remove special characters such as < , ^ etc.\n",
    "        normalize_whitespace=True,\n",
    "        fix_unicode=True,  # fix various unicode errors\n",
    "        to_ascii=True,  # transliterate to closest ASCII representation\n",
    "        lower=True,  # lowercase text\n",
    "        no_line_breaks=True,  # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=True,  # replace all URLs with a special token\n",
    "        no_emails=True,  # replace all email addresses with a special token\n",
    "        no_phone_numbers=True,  # replace all phone numbers with a special token\n",
    "        no_numbers=True,  # replace all numbers with a special token\n",
    "        no_digits=True,  # replace all digits with a special token\n",
    "        no_currency_symbols=True,  # replace all currency symbols with a special token\n",
    "        no_punct=True,  # remove punctuations\n",
    "        no_emoji=True,\n",
    "        replace_with_punct=\"\",  # instead of removing punctuations you may replace them\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        replace_with_number=\"<NUMBER>\",\n",
    "        replace_with_digit=\"<DIGIT>\",\n",
    "        replace_with_currency_symbol=\"<CUR>\",\n",
    "        lang=\"en\")\n",
    "\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_size = len(pd.read_csv('content_type_data.csv', encoding='utf8').index)\n",
    "df = pd.read_csv('content_type_data.csv', encoding='utf8', nrows=data_size, dtype={'content':'string', 'type':'string'})\n",
    "pd.DataFrame(columns=['content', 'type']).to_csv(\"cleaned_text.csv\")\n",
    "\n",
    "for i in range(0,data_size):\n",
    "    content_result = clean_data(df.iloc[i]['content'])\n",
    "    type_result = df.iloc[i]['type']\n",
    "\n",
    "    result = {'content':content_result, 'type':type_result}\n",
    "    new_data = pd.DataFrame(result, index=[i])\n",
    "    new_data.to_csv('cleaned_text.csv', mode='a', header=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('cleaned_text.csv', encoding='utf8', dtype={'content':'string', 'type':'string'})\n",
    "df_cleaned['type'] = df_cleaned['type'].replace(['unreliable', 'bias', 'junksci', 'conspiracy', 'hate', 'rumor', 'satire', 'state'], 'fake')\n",
    "df_cleaned['type'] = df_cleaned['type'].replace(['political', 'clickbait'], 'reliable')\n",
    "df_cleaned.columns = df_cleaned.columns.str.strip() #Remove unecessary \\r\n",
    "df_cleaned['type'] = df_cleaned['type'].replace('\\r', '', regex=True) #Remove unecessary r from types\n",
    "df_cleaned.to_csv('cleaned_changed_types.csv', columns=['content', 'type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def word_counter(input_text):\n",
    "    word_counter = Counter()\n",
    "    for i in input_text:\n",
    "        word_counter += Counter(word_tokenize(i[0]))\n",
    "        hej\n",
    "    return word_counter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def placeholder_word_counter(input_text, input_list):\n",
    "    placeholder_counter = [['<url>', 0], ['<email>', 0], ['<phone>', 0], ['<number>', 0], ['<digit>', 0], ['<cur>', 0]]\n",
    "\n",
    "    for i in input_text:\n",
    "        for j in range(0, len(placeholder_counter)-1):\n",
    "            placeholder_counter[j][1] += len(re.findall(placeholder_counter[j][0], i[0]))\n",
    "\n",
    "    return placeholder_counter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'function' and 'Counter'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m word_Counter \u001B[38;5;241m=\u001B[39m Counter()\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m99\u001B[39m):\n\u001B[1;32m----> 6\u001B[0m     word_counter \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m Counter(word_tokenize(df\u001B[38;5;241m.\u001B[39miloc[i][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m]))\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(word_Counter)\n",
      "\u001B[1;31mTypeError\u001B[0m: unsupported operand type(s) for +=: 'function' and 'Counter'"
     ]
    }
   ],
   "source": [
    "data_size = len(pd.read_csv('cleaned_text.csv', encoding='utf8').index)\n",
    "df = pd.read_csv('cleaned_text.csv', encoding='utf8', nrows=100, dtype={'content':'string', 'type':'string'})\n",
    "\n",
    "word_Counter = Counter()\n",
    "for i in range(0,99):\n",
    "    word_counter += Counter(word_tokenize(df.iloc[i]['content']))\n",
    "print(word_Counter)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "end_result = []\n",
    "def clean_and_store(row):\n",
    "    data_new = clean_data(row['content'])\n",
    "    end_result.append([data_new, row['type']])\n",
    "\n",
    "datasize = len(pd.read_csv('content_type_data.csv', encoding='utf8').index)\n",
    "for chunk in pd.read_csv('content_type_data.csv', encoding='utf8', nrows=datasize, chunksize=100000, dtype={'content':'string', 'type':'string'}):\n",
    "    cleaned_row_chunk = chunk.apply(clean_and_store, axis = 1)\n",
    "\n",
    "\n",
    "word_counter_result = word_counter(end_result)\n",
    "#print(word_counter_result)\n",
    "placeholder_word_counter_result = placeholder_word_counter(end_result, ['<url>', '<email>', '<phone>', '<number>', '<digit>', '<cur>'])\n",
    "#print(placeholder_word_counter_result)\n",
    "\n",
    "df_processed_end_results = pd.DataFrame(end_result, columns=['content', 'type'])\n",
    "df_processed_end_results.to_csv('cleaned_text_new.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(df_processed_end_results.index))\n",
    "\n",
    "print(placeholder_word_counter_result)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_placeholder_words(input_text):\n",
    "    cleaned_text = re.sub(r'\\<\\w+\\>', ' ', input_text)\n",
    "    cleaned_text = re.sub(' +', ' ', cleaned_text)\n",
    "\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "end_result = []\n",
    "def clean_and_store(row):\n",
    "    data_new = remove_placeholder_words(row['content'])\n",
    "    end_result.append([''.join(data_new), row['type']])\n",
    "\n",
    "datasize = len(pd.read_csv('cleaned_text_new.csv', encoding='utf8').index)\n",
    "for chunk in pd.read_csv('cleaned_text_new.csv', encoding='utf8', nrows=datasize, chunksize=100000, dtype={'content':'string', 'type':'string'}):\n",
    "    cleaned_row_chunk = chunk.apply(clean_and_store, axis = 1).dropna()\n",
    "\n",
    "word_counter_results = word_counter(end_result)\n",
    "\n",
    "df_processed_end_results = pd.DataFrame(end_result, columns=['content', 'type'])\n",
    "df_processed_end_results.to_csv('filtered_placeholder_words.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def stemming_words(input_text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    stemmed_and_filtered = [ps.stem(w) for w in word_tokenize(input_text) if w not in stop_words]\n",
    "\n",
    "    return stemmed_and_filtered"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "end_result = []\n",
    "def clean_and_store(row):\n",
    "    data_new = stemming_words(row['content'])\n",
    "    end_result.append([' '.join(data_new), row['type']])\n",
    "\n",
    "datasize = len(pd.read_csv('filtered_placeholder_words.csv', encoding='utf8').index)\n",
    "for chunk in pd.read_csv('filtered_placeholder_words.csv', encoding='utf8', nrows=datasize, chunksize=10000, dtype={'content':'string', 'type':'string'}):\n",
    "    cleaned_row_chunk = chunk.apply(clean_and_store, axis = 1).dropna()\n",
    "\n",
    "print(word_counter(end_result))\n",
    "\n",
    "df_processed_end_results = pd.DataFrame(end_result, columns=['content', 'type'])\n",
    "df_processed_end_results.to_csv('filtered_and_stemmed_words.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "remember to remove nan and unknown"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "end_result = []\n",
    "def clean_and_store(row):\n",
    "    if row['type'] in ['unreliable', 'state', 'clickbait', 'junksci', 'conspiracy', 'hate', 'rumor', 'satire']:\n",
    "        type_name = 'fake'\n",
    "    else:\n",
    "        type_name = 'reliable'\n",
    "    end_result.append([row['content'], type_name])\n",
    "\n",
    "for chunk in pd.read_csv('filtered_placeholder_words.csv', encoding='utf8', nrows=10000, chunksize=1000, dtype={'content':'string', 'type':'string'}):\n",
    "    cleaned_row_chunk = chunk.apply(clean_and_store, axis = 1).dropna()\n",
    "\n",
    "\n",
    "df_processed_end_results = pd.DataFrame(end_result, columns=['content', 'type'])\n",
    "df_processed_end_results.to_csv('removed_.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "df_processed_end_results = pd.read_csv('removed_.csv')\n",
    "\n",
    "X = df_processed_end_results['content']\n",
    "vectorizer = CountVectorizer() #Counts and vectorizes\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "y = df_processed_end_results['type']\n",
    "encoder = LabelEncoder() #Good for binary use, and sets fake as 0 and reliable as 1\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=0)\n",
    "\n",
    "#Baseline models\n",
    "DecisionTree = DecisionTreeClassifier()\n",
    "LogisticRegression = LogisticRegression(max_iter=1000)\n",
    "LinearRegression = LinearRegression()\n",
    "\n",
    "DecisionTree.fit(X_train, y_train)\n",
    "LogisticRegression.fit(X_train, y_train)\n",
    "\n",
    "y_pred_decision = DecisionTree.predict(X_test)\n",
    "y_pred_logistic = LogisticRegression.predict(X_val)\n",
    "\n",
    "acc_decision = accuracy_score(y_test, y_pred_decision)\n",
    "acc_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "\n",
    "print(acc_decision)\n",
    "print(acc_logistic)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
