{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from cleantext import clean\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import swifter\n",
    "import operator\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content without empty content:  1600000\n",
      "Content without Nan and unknown types:  1546834\n",
      "Content without duplicate contents:  1077683\n"
     ]
    }
   ],
   "source": [
    "end_result = pd.read_csv('news_cleaned_2018_02_13.csv', encoding='utf8', nrows=1_600_000, dtype={'content':'string', 'type':'string'}, usecols=['content', 'type'], lineterminator='\\n', skip_blank_lines=True)\n",
    "\n",
    "end_result.dropna(subset=['content'])\n",
    "print(\"Content without empty content:\", len(end_result.index))\n",
    "\n",
    "end_result = end_result[~end_result['type'].isin(['nan', 'unknown'])]\n",
    "print(\"Content without Nan and unknown types:\", len(end_result.index))\n",
    "\n",
    "end_result = end_result.drop_duplicates(subset=['content'])\n",
    "print(\"Content without duplicate contents:\", len(end_result.index))\n",
    "\n",
    "end_result.to_csv('content_type_data.csv', columns=['content', 'type'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def clean_data(input_text):\n",
    "    cleaned_text = re.sub(r'(\\S+\\.com*\\S+)', '<url>', input_text)\n",
    "    cleaned_text = re.sub(r'(\\S+\\.net*\\S+)', '<url>', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\-', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\|', ' ', cleaned_text)\n",
    "    cleaned_text = clean(cleaned_text,  # does not remove special characters such as < , ^ etc.\n",
    "        normalize_whitespace=True,\n",
    "        fix_unicode=True,  # fix various unicode errors\n",
    "        to_ascii=True,  # transliterate to closest ASCII representation\n",
    "        lower=True,  # lowercase text\n",
    "        no_line_breaks=True,  # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=True,  # replace all URLs with a special token\n",
    "        no_emails=True,  # replace all email addresses with a special token\n",
    "        no_phone_numbers=True,  # replace all phone numbers with a special token\n",
    "        no_numbers=True,  # replace all numbers with a special token\n",
    "        no_digits=True,  # replace all digits with a special token\n",
    "        no_currency_symbols=True,  # replace all currency symbols with a special token\n",
    "        no_punct=True,  # remove punctuations\n",
    "        no_emoji=True,\n",
    "        replace_with_punct=\"\",  # instead of removing punctuations you may replace them\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        replace_with_number=\"<NUMBER>\",\n",
    "        replace_with_digit=\"<DIGIT>\",\n",
    "        replace_with_currency_symbol=\"<CUR>\",\n",
    "        lang=\"en\")\n",
    "\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#uses 5-6 gb of ram slow 2h\n",
    "df = pd.read_csv('content_type_data.csv', encoding='utf8', dtype={'content':'string', 'type':'string'})\n",
    "data_size = len(df.index)\n",
    "pd.DataFrame(columns=['content', 'type']).to_csv(\"cleaned_text.csv\")\n",
    "\n",
    "for i in range(0,data_size):\n",
    "    content_result = clean_data(df.iloc[i]['content'])\n",
    "    type_result = df.iloc[i]['type']\n",
    "\n",
    "    result = {'content':content_result, 'type':type_result}\n",
    "    new_data = pd.DataFrame(result, index=[i])\n",
    "    new_data.to_csv('cleaned_text.csv', mode='a', header=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('cleaned_text.csv', encoding='utf8', dtype={'content':'string', 'type':'string'}).dropna(subset=['content'])\n",
    "df_cleaned['type'] = df_cleaned['type'].replace(['unreliable', 'bias', 'junksci', 'conspiracy', 'hate', 'rumor', 'satire', 'state'], 'fake')\n",
    "df_cleaned['type'] = df_cleaned['type'].replace(['political', 'clickbait'], 'reliable')\n",
    "df_cleaned.columns = df_cleaned.columns.str.strip() #Remove unecessary \\r\n",
    "df_cleaned['type'] = df_cleaned['type'].replace('\\r', '', regex=True) #Remove unecessary r from types\n",
    "df_cleaned.to_csv('cleaned_changed_types.csv', columns=['content', 'type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def remove_placeholder_words(input_text):\n",
    "    cleaned_text = re.sub(r'\\<\\w+\\>', ' ', input_text)\n",
    "    cleaned_text = re.sub(' +', ' ', cleaned_text)\n",
    "\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#runs 1gb of data in ca 30 min\n",
    "df = pd.read_csv('cleaned_changed_types.csv', encoding='utf8', dtype={'content':'string', 'type':'string'})\n",
    "data_size = len(df.index)\n",
    "pd.DataFrame(columns=['content', 'type']).to_csv(\"cleaned_removed_placeholder.csv\")\n",
    "\n",
    "for i in range(0,data_size):\n",
    "    content_result = remove_placeholder_words(df.iloc[i]['content'])\n",
    "    type_result = df.iloc[i]['type']\n",
    "\n",
    "    result = {'content':content_result, 'type':type_result}\n",
    "    new_data = pd.DataFrame(result, index=[i])\n",
    "    new_data.to_csv('cleaned_removed_placeholder.csv', mode='a', header=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_removed_placeholder.csv', encoding='utf8', dtype={'content':'string', 'type':'string'}).dropna(subset=['content'])\n",
    "df.to_csv('cleaned_removed_placeholder.csv', columns=['content', 'type'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def remove_stop_words(input_text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = []\n",
    "    cleaned_text = word_tokenize(input_text)\n",
    "\n",
    "    for w in cleaned_text:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return ' '.join(filtered_sentence)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#about 1h\n",
    "df = pd.read_csv('cleaned_removed_placeholder.csv', encoding='utf8', dtype={'content':'string', 'type':'string'}) #about as slow as clean text 1,5h\n",
    "data_size = len(df.index)\n",
    "pd.DataFrame(columns=['content', 'type']).to_csv(\"cleaned_removed_stop_words.csv\")\n",
    "\n",
    "for i in range(0,data_size):\n",
    "    content_result = remove_stop_words(df.iloc[i]['content'])\n",
    "    type_result = df.iloc[i]['type']\n",
    "\n",
    "    result = {'content':content_result, 'type':type_result}\n",
    "    new_data = pd.DataFrame(result, index=[i])\n",
    "    new_data.to_csv('cleaned_removed_stop_words.csv', mode='a', header=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_removed_stop_words.csv', encoding='utf8', dtype={'content':'string', 'type':'string'}).dropna(subset=['content'])\n",
    "df.to_csv('cleaned_removed_stop_words.csv', columns=['content', 'type'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def stemming_words(input_text):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    stemmed_words = []\n",
    "\n",
    "    for word in word_tokenize(input_text):\n",
    "        stemmed_words.append(ps.stem(word))\n",
    "\n",
    "    return ' '.join(stemmed_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#heavy on memory and cpu usage 3h\n",
    "df = pd.read_csv('cleaned_removed_stop_words.csv', encoding='utf8', dtype={'content':'string', 'type':'string'}) #about as slow as clean text 1,5h\n",
    "data_size = len(df.index)\n",
    "pd.DataFrame(columns=['content', 'type']).to_csv(\"cleaned_stemmed.csv\")\n",
    "\n",
    "for i in range(0,data_size):\n",
    "    content_result = stemming_words(df.iloc[i]['content'])\n",
    "    type_result = df.iloc[i]['type']\n",
    "\n",
    "    result = {'content':content_result, 'type':type_result}\n",
    "    new_data = pd.DataFrame(result, index=[i])\n",
    "    new_data.to_csv('cleaned_stemmed.csv', mode='a', header=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_stemmed.csv', encoding='utf8', dtype={'content':'string', 'type':'string'}).dropna(subset=['content'])\n",
    "df.to_csv('cleaned_stemmed.csv', columns=['content', 'type'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<url>', 583598], ['<email>', 4876], ['<phone>', 133847], ['<number>', 11317022], ['<digit>', 2448658], ['<cur>', 0]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_changed_types.csv', encoding='utf8', dtype={'content':'string', 'type':'string'})\n",
    "data_size = len(df.index)\n",
    "\n",
    "placeholder_counter = [['<url>', 0], ['<email>', 0], ['<phone>', 0], ['<number>', 0], ['<digit>', 0], ['<cur>', 0]]\n",
    "\n",
    "for i in range(0, data_size):\n",
    "    current_text = df.iloc[i]['content']\n",
    "    for j in range(0, len(placeholder_counter)-1):\n",
    "        placeholder_counter[j][1] += len(re.findall(placeholder_counter[j][0], current_text))\n",
    "\n",
    "print(placeholder_counter)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words after cleaning: 568003786\n",
      "Total number of words after removing placeholders: 557067402\n",
      "Total number of words after removing stopwords: 319928342\n",
      "Total number of words after stemming: 319928342\n"
     ]
    }
   ],
   "source": [
    "def total_word_counter(input_text, text_size):\n",
    "    total_counter = 0\n",
    "    for i in range(0, text_size):\n",
    "        total_counter += len(re.findall(' ', input_text.iloc[i]['content']))+1\n",
    "\n",
    "    return total_counter\n",
    "\n",
    "df = pd.read_csv('cleaned_changed_types.csv', encoding='utf8', dtype={'content': 'string', 'type': 'string'}, lineterminator='\\n')\n",
    "data_size = len(df.index)\n",
    "print(\"Total number of words after cleaning:\", total_word_counter(df, data_size))\n",
    "df = pd.read_csv('cleaned_removed_placeholder.csv', encoding='utf8', dtype={'content': 'string', 'type': 'string'}, lineterminator='\\n')\n",
    "data_size = len(df.index)\n",
    "print(\"Total number of words after removing placeholders:\", total_word_counter(df, data_size))\n",
    "df = pd.read_csv('cleaned_removed_stop_words.csv', encoding='utf8', dtype={'content': 'string', 'type': 'string'}, lineterminator='\\n')\n",
    "data_size = len(df.index)\n",
    "print(\"Total number of words after removing stopwords:\", total_word_counter(df, data_size))\n",
    "df = pd.read_csv('cleaned_stemmed.csv', encoding='utf8', dtype={'content': 'string', 'type': 'string'}, lineterminator='\\n')\n",
    "data_size = len(df.index)\n",
    "print(\"Total number of words after stemming:\", total_word_counter(df, data_size))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "not precise, because some placeholders are directly next to each other, therefore there are more placeholder counts, than fits with reduction, the reduction counts spaces, therefore <num><num> is countes as two, but counted as 1 space=word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "political     271131\n",
      "rumor         138541\n",
      "bias          136807\n",
      "fake          120177\n",
      "conspiracy    107712\n",
      "hate           42968\n",
      "unreliable     39067\n",
      "satire         28572\n",
      "clickbait      23724\n",
      "junksci        17664\n",
      "reliable        5887\n",
      "Name: type, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('content_type_data.csv', encoding='utf8', dtype={'content':'string', 'type':'string'})\n",
    "print(df['type'].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3620849), ('of', 1765584), ('to', 1747743), ('>', 1655530), ('<', 1648730), ('and', 1647538), ('a', 1293775), ('number', 1256726), ('in', 1177566), ('that', 809503), ('is', 777258), ('for', 599681), ('on', 480356), ('it', 444577), ('as', 402569), ('are', 399380), ('this', 393315), ('with', 390280), ('by', 328304), ('be', 327366), ('was', 323325), ('i', 314127), ('have', 294932), ('you', 291949), ('not', 291324), ('from', 275379), ('at', 263406), ('digit', 240903), ('he', 233062), ('they', 230941), ('we', 229491), ('but', 226207), ('has', 225617), ('or', 223807), ('an', 217466), ('his', 200019), ('will', 197488), ('their', 190004), ('all', 181030), ('its', 174714), ('who', 173780), ('more', 165041), ('about', 161656), ('us', 161195), ('one', 157972), ('can', 147928), ('if', 147790), ('what', 141692), ('were', 138496), ('so', 135376), ('our', 134629), ('which', 133456), ('would', 133415), ('been', 130153), ('people', 129191), ('your', 128260), ('out', 126080), ('trump', 125254), ('when', 122128), ('there', 119064), ('up', 115383), ('no', 113832), ('new', 108284), ('had', 107948), ('said', 107708), ('do', 100563), ('url', 100522), ('also', 98552), ('than', 97575), ('like', 96937), ('time', 96761), ('my', 96665), ('other', 96581), ('just', 94959), ('some', 92641), ('them', 92106), ('now', 88912), ('into', 88866), ('these', 88005), ('over', 86570), ('how', 81151), ('only', 80447), ('blockchain', 79819), ('after', 79063), ('even', 78542), ('two', 78315), ('most', 76390), ('any', 75982), ('state', 75351), ('president', 74532), ('because', 74112), ('many', 73412), ('her', 72660), ('first', 71574), ('years', 70321), ('those', 69145), ('government', 67921), ('next', 67431), ('could', 67105), ('world', 65517)]\n"
     ]
    }
   ],
   "source": [
    "#before stop and stem\n",
    "data_size = 100000\n",
    "df = pd.read_csv('cleaned_changed_types.csv', encoding='utf8', nrows=data_size, dtype={'content':'string', 'type':'string'})\n",
    "\n",
    "counts = dict()\n",
    "for i in range(0, data_size-1):\n",
    "    words = word_tokenize(df.iloc[i]['content'])\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "sorted_list= sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(sorted_list[0:100])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 161742), ('us', 156382), ('trump', 144752), ('state', 144295), ('year', 129830), ('peopl', 129733), ('would', 128044), ('time', 125279), ('like', 115540), ('said', 110358), ('new', 105456), ('use', 98042), ('also', 96592), ('report', 91957), ('make', 87900), ('presid', 86329), ('get', 83634), ('american', 82077), ('blockchain', 79879), ('even', 79105), ('say', 78881), ('go', 78830), ('govern', 78781), ('two', 76646), ('right', 75805), ('nation', 70983), ('mani', 70949), ('world', 70801), ('day', 70366), ('work', 70096), ('first', 69124), ('think', 68333), ('next', 66444), ('could', 64730), ('take', 63794), ('may', 63319), ('obama', 62196), ('come', 62041), ('know', 61975), ('way', 61940), ('need', 61065), ('call', 61063), ('market', 60649), ('well', 60380), ('see', 59110), ('want', 57778), ('countri', 54915), ('includ', 54139), ('democrat', 53076), ('support', 52281), ('news', 51851), ('republican', 51565), ('fact', 51522), ('sourc', 51246), ('stock', 51149), ('polit', 51101), ('law', 51056), ('last', 50921), ('back', 50854), ('thing', 50702), ('hous', 50430), ('unit', 49662), ('dont', 49503), ('war', 49288), ('much', 48971), ('live', 48508), ('search', 47076), ('help', 46658), ('look', 46551), ('power', 46408), ('stori', 45516), ('talk', 45245), ('show', 44989), ('good', 44702), ('group', 44667), ('america', 44530), ('bitcoin', 44077), ('public', 43699), ('headlin', 43593), ('elect', 43150), ('made', 42319), ('chang', 42197), ('continu', 41995), ('life', 41881), ('follow', 41174), ('exceed', 40950), ('end', 40080), ('post', 40069), ('point', 39781), ('vote', 39698), ('part', 39655), ('read', 39113), ('gener', 39010), ('person', 38808), ('week', 38759), ('sinc', 38643), ('parti', 38405), ('million', 38159), ('forc', 38136), ('plan', 38021)]\n"
     ]
    }
   ],
   "source": [
    "#after stop and stem\n",
    "data_size = 100000\n",
    "df = pd.read_csv('cleaned_stemmed.csv', encoding='utf8', nrows=data_size, dtype={'content':'string', 'type':'string'})\n",
    "\n",
    "\n",
    "counts = dict()\n",
    "for i in range(0, data_size-1):\n",
    "    words = word_tokenize(df.iloc[i]['content'])\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "sorted_list= sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(sorted_list[0:100])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headers:  ['Unnamed: 0' 'id' 'domain' 'type' 'url' 'content' 'scraped_at'\n",
      " 'inserted_at' 'updated_at' 'title' 'authors' 'keywords' 'meta_keywords'\n",
      " 'meta_description' 'tags' 'summary' 'source']\n",
      "amount of headers:  17\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('news_cleaned_2018_02_13.csv', nrows=0, encoding='utf8')\n",
    "print(\"headers: \", df.columns.values)\n",
    "print(\"amount of headers: \", len(df.columns.values))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NAN in colum Unnamed: 0 : 0\n",
      "Number of NAN in colum id : 0\n",
      "Number of NAN in colum domain : 0\n",
      "Number of NAN in colum type : 169748\n",
      "Number of NAN in colum url : 0\n",
      "Number of NAN in colum content : 0\n",
      "Number of NAN in colum scraped_at : 0\n",
      "Number of NAN in colum inserted_at : 0\n",
      "Number of NAN in colum updated_at : 0\n",
      "Number of NAN in colum title : 15678\n",
      "Number of NAN in colum authors : 703210\n",
      "Number of NAN in colum keywords : 1600000\n",
      "Number of NAN in colum meta_keywords : 0\n",
      "Number of NAN in colum meta_description : 948698\n",
      "Number of NAN in colum tags : 963113\n",
      "Number of NAN in colum summary : 1600000\n",
      "Number of NAN in colum source : 1600000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('news_cleaned_2018_02_13.csv', nrows=1_600_000, encoding='utf8', dtype={'Unnamed: 0': 'string', 'id': 'string', 'domain': 'string', 'type': 'string', 'url': 'string', 'content': 'string', 'scraped_at': 'string', 'inserted_at': 'string', 'updated_at': 'string', 'title': 'string', 'authors': 'string', 'keywords': 'string', 'meta_keywords': 'string','meta_description': 'string', 'tags': 'string', 'summary': 'string', 'source': 'string',}, lineterminator='\\n')\n",
    "\n",
    "colum_names = df.columns.values\n",
    "for i in range(0, len(colum_names)):\n",
    "    nan_count = df[str(colum_names[i])].isna().sum()\n",
    "    print(\"Number of NAN in colum \" + str(colum_names[i]) + \":\", nan_count)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
