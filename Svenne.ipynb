{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from cleantext import clean\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content without empty:  1600000\n",
      "Content without Nan and unknown:  1546834\n",
      "Content without duplicates:  1077683\n"
     ]
    }
   ],
   "source": [
    "end_result = pd.read_csv('news_cleaned_2018_02_13.csv', encoding='utf8', nrows=1_600_000, dtype={'content':'string', 'type':'string'}, usecols=['content', 'type'], lineterminator='\\n', skip_blank_lines=True).dropna(subset=['content'])\n",
    "print(\"Content without empty: \", len(end_result.index))\n",
    "end_result = end_result[~end_result['type'].isin(['nan', 'unknown'])]\n",
    "print(\"Content without Nan and unknown: \", len(end_result.index))\n",
    "end_result = end_result.drop_duplicates(subset=['content'])\n",
    "print(\"Content without duplicates: \", len(end_result.index))\n",
    "end_result.to_csv('content_type_data.csv', columns=['content', 'type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def clean_data(input_text):\n",
    "    cleaned_text = re.sub(r'(\\S+\\.com*\\S+)', '<url>', input_text)\n",
    "    cleaned_text = re.sub(r'(\\S+\\.net*\\S+)', '<url>', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\-', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\|', ' ', cleaned_text)\n",
    "    cleaned_text = clean(cleaned_text,  # does not remove special characters such as < , ^ etc.\n",
    "        normalize_whitespace=True,\n",
    "        fix_unicode=True,  # fix various unicode errors\n",
    "        to_ascii=True,  # transliterate to closest ASCII representation\n",
    "        lower=True,  # lowercase text\n",
    "        no_line_breaks=True,  # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=True,  # replace all URLs with a special token\n",
    "        no_emails=True,  # replace all email addresses with a special token\n",
    "        no_phone_numbers=True,  # replace all phone numbers with a special token\n",
    "        no_numbers=True,  # replace all numbers with a special token\n",
    "        no_digits=True,  # replace all digits with a special token\n",
    "        no_currency_symbols=True,  # replace all currency symbols with a special token\n",
    "        no_punct=True,  # remove punctuations\n",
    "        no_emoji=True,\n",
    "        replace_with_punct=\"\",  # instead of removing punctuations you may replace them\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        replace_with_number=\"<NUMBER>\",\n",
    "        replace_with_digit=\"<DIGIT>\",\n",
    "        replace_with_currency_symbol=\"<CUR>\",\n",
    "        lang=\"en\")\n",
    "\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#uses 5-6 gb of ram slow 2h\n",
    "df = pd.read_csv('content_type_data.csv', encoding='utf8', dtype={'content':'string', 'type':'string'})\n",
    "data_size = len(df.index)\n",
    "pd.DataFrame(columns=['content', 'type']).to_csv(\"cleaned_text.csv\")\n",
    "\n",
    "for i in range(0,data_size):\n",
    "    content_result = clean_data(df.iloc[i]['content'])\n",
    "    type_result = df.iloc[i]['type']\n",
    "\n",
    "    result = {'content':content_result, 'type':type_result}\n",
    "    new_data = pd.DataFrame(result, index=[i])\n",
    "    new_data.to_csv('cleaned_text.csv', mode='a', header=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('cleaned_text.csv', encoding='utf8', dtype={'content':'string', 'type':'string'}).dropna(subset=['content'])\n",
    "df_cleaned['type'] = df_cleaned['type'].replace(['unreliable', 'bias', 'junksci', 'conspiracy', 'hate', 'rumor', 'satire', 'state'], 'fake')\n",
    "df_cleaned['type'] = df_cleaned['type'].replace(['political', 'clickbait'], 'reliable')\n",
    "df_cleaned.columns = df_cleaned.columns.str.strip() #Remove unecessary \\r\n",
    "df_cleaned['type'] = df_cleaned['type'].replace('\\r', '', regex=True) #Remove unecessary r from types\n",
    "df_cleaned.to_csv('cleaned_changed_types.csv', columns=['content', 'type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def remove_placeholder_words(input_text):\n",
    "    cleaned_text = re.sub(r'\\<\\w+\\>', ' ', input_text)\n",
    "    cleaned_text = re.sub(' +', ' ', cleaned_text)\n",
    "\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#runs 1gb of data in ca 30 min\n",
    "df = pd.read_csv('cleaned_changed_types.csv', encoding='utf8', dtype={'content':'string', 'type':'string'})\n",
    "data_size = len(df.index)\n",
    "pd.DataFrame(columns=['content', 'type']).to_csv(\"cleaned_removed_placeholder.csv\")\n",
    "\n",
    "for i in range(0,data_size):\n",
    "    content_result = remove_placeholder_words(df.iloc[i]['content'])\n",
    "    type_result = df.iloc[i]['type']\n",
    "\n",
    "    result = {'content':content_result, 'type':type_result}\n",
    "    new_data = pd.DataFrame(result, index=[i])\n",
    "    new_data.to_csv('cleaned_removed_placeholder.csv', mode='a', header=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1077038\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_removed_placeholder.csv', encoding='utf8', dtype={'content':'string', 'type':'string'})\n",
    "print(len(df.index))\n",
    "df.dropna(subset=['content'])\n",
    "print(len(df.index))\n",
    "df.to_csv('cleaned_removed_placeholder.csv', columns=['content', 'type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def remove_stop_words(input_text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = []\n",
    "    cleaned_text = word_tokenize(input_text)\n",
    "\n",
    "    for w in cleaned_text:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return ' '.join(filtered_sentence)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#about 1h\n",
    "df = pd.read_csv('cleaned_removed_placeholder.csv', encoding='utf8', dtype={'content':'string', 'type':'string'}) #about as slow as clean text 1,5h\n",
    "data_size = len(df.index)\n",
    "pd.DataFrame(columns=['content', 'type']).to_csv(\"cleaned_removed_stop_words.csv\")\n",
    "\n",
    "for i in range(0,data_size):\n",
    "    content_result = remove_stop_words(df.iloc[i]['content'])\n",
    "    type_result = df.iloc[i]['type']\n",
    "\n",
    "    result = {'content':content_result, 'type':type_result}\n",
    "    new_data = pd.DataFrame(result, index=[i])\n",
    "    new_data.to_csv('cleaned_removed_stop_words.csv', mode='a', header=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1075189\n",
      "1075189\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_removed_stop_words.csv', encoding='utf8', dtype={'content':'string', 'type':'string'}).dropna(subset=['content'])\n",
    "print(len(df.index))\n",
    "df.to_csv('cleaned_removed_stop_words.csv', columns=['content', 'type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def stemming_words(input_text):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    stemmed_words = []\n",
    "\n",
    "    for word in word_tokenize(input_text):\n",
    "        stemmed_words.append(ps.stem(word))\n",
    "\n",
    "    return ' '.join(stemmed_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#heavy on memory and cpu usage 3h\n",
    "df = pd.read_csv('cleaned_removed_stop_words.csv', encoding='utf8', dtype={'content':'string', 'type':'string'}) #about as slow as clean text 1,5h\n",
    "data_size = len(df.index)\n",
    "pd.DataFrame(columns=['content', 'type']).to_csv(\"cleaned_stemmed.csv\")\n",
    "\n",
    "for i in range(0,data_size):\n",
    "    content_result = stemming_words(df.iloc[i]['content'])\n",
    "    type_result = df.iloc[i]['type']\n",
    "\n",
    "    result = {'content':content_result, 'type':type_result}\n",
    "    new_data = pd.DataFrame(result, index=[i])\n",
    "    new_data.to_csv('cleaned_stemmed.csv', mode='a', header=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1075189\n",
      "1075189\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_stemmed.csv', encoding='utf8', dtype={'content':'string', 'type':'string'})\n",
    "print(len(df.index))\n",
    "df.dropna(subset=['content'])\n",
    "print(len(df.index))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "everything works upto here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def placeholder_word_counter(input_text, input_list):\n",
    "    placeholder_counter = [['<url>', 0], ['<email>', 0], ['<phone>', 0], ['<number>', 0], ['<digit>', 0], ['<cur>', 0]]\n",
    "\n",
    "    for i in input_text:\n",
    "        for j in range(0, len(placeholder_counter)-1):\n",
    "            placeholder_counter[j][1] += len(re.findall(placeholder_counter[j][0], i[0]))\n",
    "\n",
    "    return placeholder_counter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def word_counter(input_text):\n",
    "    word_counter = Counter()\n",
    "    for i in input_text:\n",
    "        word_counter += Counter(word_tokenize(i[0]))\n",
    "\n",
    "    return word_counter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_size = len(pd.read_csv('cleaned_changed_types.csv', encoding='utf8').index)\n",
    "df = pd.read_csv('cleaned_changed_types.csv', encoding='utf8', nrows=100000, dtype={'content':'string', 'type':'string'})\n",
    "s = pd.Series(df['content']).str.split().value_counts()[:100] #uses up to 9gb of ram for 100k\n",
    "#s = s.str.split()\n",
    "#s.columns = ['content']\n",
    "print(s)\n",
    "\n",
    "\n",
    "\"\"\"word_counter = Counter()\n",
    "for i in range(0, data_size):\n",
    "    if i%50000 == 0:\n",
    "        print(\"hej\")\n",
    "    count = things.groupby(['colors']).size()\n",
    "    word_counter += Counter(word_tokenize(df.iloc[i]['content']))\n",
    "print(word_counter)\"\"\"\n",
    "\n",
    "\"\"\"new_df = df['content'].str.split(expand=True).stack().value_counts().reset_index()\n",
    "\n",
    "new_df.columns = ['Word', 'Frequency']\n",
    "print(new_df)\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "end_result = []\n",
    "def clean_and_store(row):\n",
    "    data_new = remove_placeholder_words(row['content'])\n",
    "    end_result.append([''.join(data_new), row['type']])\n",
    "\n",
    "datasize = len(pd.read_csv('cleaned_text_new.csv', encoding='utf8').index)\n",
    "for chunk in pd.read_csv('cleaned_text_new.csv', encoding='utf8', nrows=datasize, chunksize=100000, dtype={'content':'string', 'type':'string'}):\n",
    "    cleaned_row_chunk = chunk.apply(clean_and_store, axis = 1).dropna()\n",
    "\n",
    "word_counter_results = word_counter(end_result)\n",
    "\n",
    "df_processed_end_results = pd.DataFrame(end_result, columns=['content', 'type'])\n",
    "df_processed_end_results.to_csv('filtered_placeholder_words.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "end_result = []\n",
    "def clean_and_store(row):\n",
    "    data_new = clean_data(row['content'])\n",
    "    end_result.append([data_new, row['type']])\n",
    "\n",
    "datasize = len(pd.read_csv('content_type_data.csv', encoding='utf8').index)\n",
    "for chunk in pd.read_csv('content_type_data.csv', encoding='utf8', nrows=datasize, chunksize=100000, dtype={'content':'string', 'type':'string'}):\n",
    "    cleaned_row_chunk = chunk.apply(clean_and_store, axis = 1)\n",
    "\n",
    "\n",
    "word_counter_result = word_counter(end_result)\n",
    "#print(word_counter_result)\n",
    "placeholder_word_counter_result = placeholder_word_counter(end_result, ['<url>', '<email>', '<phone>', '<number>', '<digit>', '<cur>'])\n",
    "#print(placeholder_word_counter_result)\n",
    "\n",
    "df_processed_end_results = pd.DataFrame(end_result, columns=['content', 'type'])\n",
    "df_processed_end_results.to_csv('cleaned_text_new.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "end_result = []\n",
    "def clean_and_store(row):\n",
    "    data_new = stemming_words(row['content'])\n",
    "    end_result.append([' '.join(data_new), row['type']])\n",
    "\n",
    "datasize = len(pd.read_csv('filtered_placeholder_words.csv', encoding='utf8').index)\n",
    "for chunk in pd.read_csv('filtered_placeholder_words.csv', encoding='utf8', nrows=datasize, chunksize=10000, dtype={'content':'string', 'type':'string'}):\n",
    "    cleaned_row_chunk = chunk.apply(clean_and_store, axis = 1).dropna()\n",
    "\n",
    "print(word_counter(end_result))\n",
    "\n",
    "df_processed_end_results = pd.DataFrame(end_result, columns=['content', 'type'])\n",
    "df_processed_end_results.to_csv('filtered_and_stemmed_words.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "end_result = []\n",
    "def clean_and_store(row):\n",
    "    if row['type'] in ['unreliable', 'state', 'clickbait', 'junksci', 'conspiracy', 'hate', 'rumor', 'satire']:\n",
    "        type_name = 'fake'\n",
    "    else:\n",
    "        type_name = 'reliable'\n",
    "    end_result.append([row['content'], type_name])\n",
    "\n",
    "for chunk in pd.read_csv('filtered_placeholder_words.csv', encoding='utf8', nrows=10000, chunksize=1000, dtype={'content':'string', 'type':'string'}):\n",
    "    cleaned_row_chunk = chunk.apply(clean_and_store, axis = 1).dropna()\n",
    "\n",
    "\n",
    "df_processed_end_results = pd.DataFrame(end_result, columns=['content', 'type'])\n",
    "df_processed_end_results.to_csv('removed_.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "df_processed_end_results = pd.read_csv('removed_.csv')\n",
    "\n",
    "X = df_processed_end_results['content']\n",
    "vectorizer = CountVectorizer() #Counts and vectorizes\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "y = df_processed_end_results['type']\n",
    "encoder = LabelEncoder() #Good for binary use, and sets fake as 0 and reliable as 1\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=0)\n",
    "\n",
    "#Baseline models\n",
    "DecisionTree = DecisionTreeClassifier()\n",
    "LogisticRegression = LogisticRegression(max_iter=1000)\n",
    "LinearRegression = LinearRegression()\n",
    "\n",
    "DecisionTree.fit(X_train, y_train)\n",
    "LogisticRegression.fit(X_train, y_train)\n",
    "\n",
    "y_pred_decision = DecisionTree.predict(X_test)\n",
    "y_pred_logistic = LogisticRegression.predict(X_val)\n",
    "\n",
    "acc_decision = accuracy_score(y_test, y_pred_decision)\n",
    "acc_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "\n",
    "print(acc_decision)\n",
    "print(acc_logistic)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
